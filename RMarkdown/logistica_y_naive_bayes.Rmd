---
title: "Regresión Logística y Naive Bayes"
output: html_notebook
---

# Bibliotecas

```{r}
library(tidyverse)
library(caret)
library(pROC)

source("../R/Utils.R")
```

# Variables Globales

```{r}
PATH_DATOS <- "../data/datos_eda.csv"
SEP_COLUMNAS <- ";"
SEP_DECIMAL <- "."

SEED <- 987
PORCENTAJE_TRAIN <- .8
```

# Cargar datos

```{r}
datos_crudos <- read.table(PATH_DATOS, sep = SEP_COLUMNAS, dec = SEP_DECIMAL, header = TRUE)
glimpse(datos_crudos)
```

Primero transfromamos todas las variables del tipo character como factores

```{r}

datos <- datos_crudos %>% 
  map_dfc(~ if(is.character(.x)) { factor(.x) } else { .x})

glimpse(datos)

```

Observamos la cantidad de NAs en cada variable

```{r}
datos %>% map_int(~ sum(is.na(.x)))
```
# Regresión Logística

Vamos a armar una regresión logística con diagnostico_pro como respuesta y las variables d1, d2, d3, d4, d5 y edad como predictoras.

```{r}
datos_log <- datos %>%
  select(diagnostico_pro, d1, d2, d3, d4, d5, edad)

glimpse(datos_log)
```
Dividimos los datos

```{r}
set.seed(SEED)

indices_train_log <- createDataPartition(datos_log$diagnostico_pro, 
                                         p = PORCENTAJE_TRAIN, 
                                         list = FALSE) %>% 
  as.vector()

datos_log_train <- datos_log[indices_train_log,]
datos_log_test <- datos_log[-indices_train_log,]

nrow(datos_log_train)
nrow(datos_log_test)
```
Entrenamos el modelo con las particiones

```{r}
modelo_log <- train(diagnostico_pro ~ .,
                    data = datos_log_train,
                    method = "glm",
                    family= "binomial")

summary(modelo_log)
```

Observamos que solamente las variables d1, d2 y edad figuran con un pvalor significativo. Interpretamos uno de los resultados:

  * Por cada incremento de un año en la edad, hay un incremento promedio de `r round((exp(0.110494) - 1) * 100, 3)`% en los odds de tener un diagnóstico postivo de EP, manteniendo el resto de variables constantes.
  
Con el modelo elegido, vamos armar la curva ROC.

```{r}
graficar_curva_roc(modelo_log, datos_log_test, nivel_positivo = "true", var_respuesta = "diagnostico_pro")
```

Vamos a tratar de mejorar el AUC identificando observaciones influyentes para el modelo. Usamos la distancia de cook como métrica.


```{r}
data.frame(d_cook = cooks.distance(modelo_log$finalModel)) %>% arrange(- d_cook)
```

Observamos que la observación 302 es la que se encuentra más alejada del resto, por lo que vamos a probar entrenando otro modelo sin esa observación. 

```{r}
nro_fila_influyente <- 302

# Observamos como es ese registro
datos_log[nro_fila_influyente,]

# Excluimos la observacion
datos_log_sin_inf <- datos_log[-nro_fila_influyente,]

# Realizamos la particion de vuelta
set.seed(SEED)
indices_train_inf <- createDataPartition(datos_log_sin_inf$diagnostico_pro, 
                                         p = PORCENTAJE_TRAIN, 
                                         list = FALSE) %>% 
  as.vector()

datos_log_inf_train <- datos_log_sin_inf[indices_train_inf,]
datos_log_inf_test <- datos_log_sin_inf[- indices_train_inf,]

# Verifiquemos que se cree correctamente
nrow(datos_log_inf_train)
nrow(datos_log_inf_test)

```

Habiendo hecho la partición, ajustamos el modelo

```{r}
modelo_log_inf <- train(diagnostico_pro ~ .,
                        datos_log_inf_train,
                        method = "glm",
                        family = "binomial")

summary(modelo_log_inf)
```

Comparando con el primer modelo de logístico armado, los coeficientes son similares.

Armamos de nuevo la curva ROC

```{r}
graficar_curva_roc(modelo_log_inf, datos_log_inf_test, "true", "diagnostico_pro")
```

Se observa que el AUC es el mismo, así que no excluimos esa observación. 

Ahora lo que hacemos es graficar specificity vs sensitivity para todos los posibles puntos de corte

```{r}
# Primero creamos un objeto roc
predicciones_log <- predict(modelo_log, datos_log_test, list = FALSE, type = "prob")
objeto_roc_log <- roc(datos_log_test$diagnostico_pro, predicciones_log[, "true"])


# Armamos el grafico
data.frame(Specificity = objeto_roc_log$specificities,
           Sencitivity = objeto_roc_log$sensitivities,
           Thresholds = objeto_roc_log$thresholds) %>% 
  ggplot(aes(x = Thresholds)) +
  geom_line(aes(y = Specificity), color = "firebrick") +
  geom_line(aes(y = Sencitivity), color = "orange") + 
  geom_vline(xintercept = .5, color = "gray") +
  geom_vline(xintercept = .4, color = "gray") +
  ylab("")
```
Observamos que el punto de intersección se encuentra en algún punto entre 0.4 y 0.5. Elegimos el punto 0.44 como punto de corte para armar una matriz de confusión.

```{r}

# Hacemos las predicciones usando 0.44 como punto de corte
prediccioes_log_matriz_confusion <- ifelse(predicciones_log[, "true"] >= 0.44,
                                           "true",
                                           "false") %>%
  as.factor()

# Armamos la matriz de confusion
confusionMatrix(prediccioes_log_matriz_confusion, 
                datos_log_test$diagnostico_pro, 
                positive = "true")

```


